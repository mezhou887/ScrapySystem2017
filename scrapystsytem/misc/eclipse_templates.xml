<?xml version="1.0" encoding="UTF-8" standalone="no"?><templates><template autoinsert="true" context="org.python.pydev.editor.templates.python.modules" deleted="false" description="高级爬虫1: 以Cnbeta作为模板开发而成" enabled="true" name="CrawlerSpider1"># -*- coding: utf-8 -*-&#13;
'''&#13;
Created on ${date}&#13;
&#13;
@author: ${user}&#13;
'''&#13;
import logging&#13;
import scrapy&#13;
from scrapy.spiders import Rule, CrawlSpider&#13;
from scrapy.linkextractors import LinkExtractor as sle&#13;
from scrapy.selector import Selector&#13;
from scrapystsytem.items import AaaaItem&#13;
&#13;
logger = logging.getLogger(__name__)&#13;
&#13;
#  以Cnbeta作为模板，使用CrawlSpider&#13;
class AaaaSpider(CrawlSpider):&#13;
    name = 'cnbeta'&#13;
    allowed_domains = ['cnbeta.com']&#13;
    start_urls = [&#13;
        'http://www.cnbeta.com/'&#13;
    ]&#13;
     &#13;
    rules = [&#13;
        Rule(sle(allow=("/articles/.*\.htm")), callback='parse_bbbb', follow=True),&#13;
    ]&#13;
    &#13;
    def parse_bbbb(self, response):&#13;
    	logger.debug('parse_bbbb: '+response.url);&#13;
        articlelist = Selector(response).xpath('//div[@class="cnbeta-article"]');&#13;
        items = [];&#13;
        for article in articlelist: &#13;
            item = AaaaItem();&#13;
            item['title'] = article.xpath('header[@class="title"]/h1/text()').extract_first();&#13;
            item['publishtime']  = article.xpath('header[@class="title"]/div[@class="meta"]/span/text()').extract_first();&#13;
            &#13;
            logger.info('function: parse_bbbb, url: '+response.url+' , item: '+str(item));&#13;
            items.append(item);&#13;
        return items;</template><template autoinsert="true" context="org.python.pydev.editor.templates.python.modules" deleted="false" description="高级爬虫2: 以豆瓣电影作为模板" enabled="true" name="CrawlerSpider2"># -*- coding: utf-8 -*-&#13;
'''&#13;
Created on ${date}&#13;
&#13;
@author: ${user}&#13;
'''&#13;
import logging&#13;
import scrapy&#13;
from scrapystsytem.misc.commonspider import CommonSpider&#13;
from scrapy.spiders import Rule&#13;
from scrapy.linkextractors import LinkExtractor as sle&#13;
&#13;
logger = logging.getLogger(__name__)&#13;
&#13;
# 以豆瓣电影作为模板，继承的CommonSpider使用CrawlSpider&#13;
class AaaaSpider(CommonSpider):&#13;
    name = "doubanmovie"&#13;
    allowed_domains = ["douban.com"]&#13;
    start_urls = [&#13;
        "https://movie.douban.com/chart"&#13;
    ]&#13;
&#13;
    rules = [&#13;
        Rule(sle(allow=("/subject/[0-9]+/")), callback='parse_bbbb', follow=True),&#13;
    ]&#13;
&#13;
    content_css_rules = {&#13;
        'rating_per': '.rating_per::text',&#13;
        'rating_num': '.rating_num::text',&#13;
        'title': 'h1 span:nth-child(1)::text',&#13;
        'rating_people': '.rating_people span::text',&#13;
    }&#13;
&#13;
    def parse_bbbb(self, response):&#13;
        item = self.parse_with_rules(response, self.content_css_rules, dict)&#13;
        logger.info('function: parse_bbbb, url: '+response.url+' , item: '+str(item));&#13;
        return item</template><template autoinsert="true" context="org.python.pydev.editor.templates.python" deleted="false" description="JSON Pipeline模板1" enabled="true" name="JsonPipeline1"># -*- coding: utf-8 -*-&#13;
import json&#13;
import codecs&#13;
from collections import OrderedDict&#13;
from scrapy.exceptions import DropItem&#13;
&#13;
class AaaaJsonPipeline(object):&#13;
    max_dropcount = 10         # 抓取数量&#13;
    current_dropcount = 0      # 当前数量&#13;
 &#13;
    def __init__(self):&#13;
        self.file = codecs.open('Aaaa.json', 'w', encoding='utf-8')&#13;
 &#13;
    def process_item(self, item, spider):&#13;
        self.current_dropcount += 1&#13;
        if(self.current_dropcount &gt;= self.max_dropcount):&#13;
            spider.close_down = True&#13;
            raise DropItem("reach max limit")        &#13;
&#13;
        line = json.dumps(OrderedDict(item), ensure_ascii=False, sort_keys=False) + "\n"&#13;
        self.file.write(line)&#13;
        return item&#13;
 &#13;
    def spider_closed(self, spider):&#13;
        self.file.close();&#13;
		&#13;
# from scrapy.exceptions import CloseSpider&#13;
# close_down = False&#13;
# if(self.close_down == True):&#13;
#     raise CloseSpider(reason = "达到抓取数量")  </template><template autoinsert="true" context="org.python.pydev.editor.templates.python" deleted="false" description="" enabled="true" name="MongoPipeline1"># -*- coding: utf-8 -*-&#13;
from pymongo import MongoClient&#13;
from scrapy.exceptions import DropItem&#13;
&#13;
class AaaaMongoPipeline(object):&#13;
    collection_zhihuuser = 'users'&#13;
    collection_proxy = 'proxy'&#13;
    max_dropcount = 1000000  # 抓取数量&#13;
    current_dropcount = 0    # 当前数量&#13;
&#13;
    def __init__(self, mongo_server, mongo_port, mongo_db, mongo_user, mongo_passwd):&#13;
        self.mongo_server = mongo_server&#13;
        self.mongo_port = mongo_port&#13;
        self.mongo_db = mongo_db&#13;
        self.mongo_user = mongo_user&#13;
        self.mongo_passwd = mongo_passwd&#13;
&#13;
    @classmethod&#13;
    def from_crawler(cls, crawler):&#13;
        return cls(&#13;
            mongo_server=crawler.settings.get('MONGODB_SERVER'),&#13;
            mongo_port=crawler.settings.get('MONGODB_PORT'),&#13;
            mongo_db=crawler.settings.get('MONGODB_DB'),&#13;
            mongo_user=crawler.settings.get('MONGO_USER'),&#13;
            mongo_passwd=crawler.settings.get('MONGO_PASSWD')&#13;
        )&#13;
&#13;
    def open_spider(self, spider):&#13;
        uri = "mongodb://%s:%s@%s:%s" % (self.mongo_user, self.mongo_passwd, self.mongo_server, self.mongo_port)&#13;
        self.client = MongoClient(uri)&#13;
        self.db = self.client[self.mongo_db]&#13;
&#13;
    def close_spider(self, spider):&#13;
        self.client.close()&#13;
&#13;
    def process_item(self, item, spider):&#13;
        self.current_dropcount += 1&#13;
        if(self.current_dropcount &gt;= self.max_dropcount):&#13;
            spider.close_down = True&#13;
            raise DropItem("reach max limit")&#13;
        &#13;
        if "zhihuuser" == spider.name: &#13;
            # 第一个参数传入查询条件，这里使用的是url_token，&#13;
            # 第二个参数传入字典类型的对象，就是我们的item，&#13;
            # 第三个参数传入True，这样就可以保证去重，如果查询数据存在的话就更新，不存在的话就插入。&#13;
            self.db[self.collection_zhihuuser].update({'url_token': item['url_token']}, {'美元符set': dict(item)}, True)&#13;
        elif "xicidaili" == spider.name:&#13;
            self.db[self.collection_proxy].insert(dict(item))    &#13;
        return item&#13;
		&#13;
# from scrapy.exceptions import CloseSpider&#13;
# close_down = False&#13;
# if(self.close_down == True):&#13;
#     raise CloseSpider(reason = "达到抓取数量")    &#13;
# ITEM_PIPELINES = {&#13;
#     'zhihusystem.pipelines.MongoPipeline': 300,&#13;
#}</template><template autoinsert="true" context="org.python.pydev.editor.templates.python.modules" deleted="false" description="基本爬虫1: 以码农周刊作为模板开发而成的" enabled="true" name="Spider1"># -*- coding: utf-8 -*-&#13;
'''&#13;
Created on ${date}&#13;
&#13;
@author: ${user}&#13;
'''&#13;
import logging&#13;
import scrapy&#13;
from scrapy.spiders import Spider&#13;
from scrapy.selector import Selector&#13;
from scrapystsytem.items import AaaaItem&#13;
&#13;
logger = logging.getLogger(__name__)&#13;
&#13;
# 以码农周刊作为模板，使用spider&#13;
# start_urls = ['http://jandan.net/ooxx/page-'+str(i)+'#comments' for i in range(1500, 2000)]&#13;
# &#13;
class AaaaSpider(Spider):&#13;
    name = 'manong'&#13;
    allowed_domains = ['weekly.manong.io']&#13;
    start_urls = [&#13;
        'http://weekly.manong.io/issues/'+str(i) for i in range(1, 200)&#13;
    ]&#13;
    &#13;
    def parse(self, response):&#13;
        sel = Selector(response)&#13;
        items = []&#13;
        article_list  = sel.xpath('//h4')&#13;
        for article in article_list:&#13;
            item         = AaaaItem()&#13;
            item['name'] = article.xpath('a/text()').extract()[0].strip()&#13;
            item['link'] = article.xpath('a/@href').extract()[0].strip()&#13;
            &#13;
            logger.info('function: parse, url: '+response.url+' , item: '+str(item));&#13;
            items.append(item)&#13;
            &#13;
        return items   </template><template autoinsert="true" context="org.python.pydev.editor.templates.python.modules" deleted="false" description="基本爬虫2: 以QuoteSpider作为模板开发而成的" enabled="true" name="Spider2"># -*- coding: utf-8 -*-&#13;
'''&#13;
Created on ${date}&#13;
&#13;
@author: ${user}&#13;
'''&#13;
import logging&#13;
import scrapy&#13;
from scrapy.spiders import Spider&#13;
from scrapy.selector import Selector&#13;
from scrapystsytem.items import AaaaItem&#13;
&#13;
logger = logging.getLogger(__name__)&#13;
&#13;
# 以QuoteSpider作为模板，使用spider&#13;
#&#13;
class AaaaSpider(Spider):&#13;
    name = 'quites';&#13;
    start_urls = [&#13;
        'http://quotes.toscrape.com/page/1/',&#13;
        'http://quotes.toscrape.com/page/2/',&#13;
    ];&#13;
&#13;
    def parse(self, response):&#13;
        for quote in response.css('div.quote'):&#13;
            yield {&#13;
                'text': quote.css('span.text::text').extract_first(),&#13;
                'author': quote.css('small.author::text').extract_first(),&#13;
                'tags': quote.css('div.tags a.tag::text').extract(),&#13;
            }&#13;
&#13;
    def closed(self, reason):&#13;
        print 'closed', reason</template></templates>